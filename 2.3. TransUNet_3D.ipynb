{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEHvEmfo906K"
   },
   "source": [
    "# 1. TransUNet for 3D Medical Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjHPEhyk906M"
   },
   "source": [
    "## Feature Size for Concatenation : 1/2, 1/4, 1/8\n",
    "## Feature Size for Embedding : 1/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34e36534"
   },
   "source": [
    "HyperParameter\n",
    "\n",
    "- Batch Size\n",
    "\n",
    "- Learning Rate\n",
    "\n",
    "- Loss Weights Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9439,
     "status": "ok",
     "timestamp": 1630633795712,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "jqugVKvU906N",
    "outputId": "f4c7559a-d133-4d93-b000-60f6abea57b7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv3D, BatchNormalization, ReLU, UpSampling3D\n",
    "\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "#!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#!pip install pydicom\n",
    "import pydicom as dcm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "#!pip install opencv-python\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-multi-head-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1630633795713,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "x4lx8nrq906O"
   },
   "outputs": [],
   "source": [
    "# 1. Data\n",
    "image_width, image_height, image_depth = 512//2, 512//2, 64\n",
    "\n",
    "image_channel = 1\n",
    "\n",
    "base_dir = \"body-morphometry-kidney-and-tumor/\"\n",
    "save_dir = \"C:/Users/user/Desktop/body-morphometry-kidney-and-tumor/train/\"\n",
    "\n",
    "# 2. Model\n",
    "# 2.1. Transformer\n",
    "featuremap_size =  (image_width// 16, image_height// 16, image_depth // 16)\n",
    "\n",
    "patch_size = 1\n",
    "num_patches = featuremap_size[0] * featuremap_size[1] * featuremap_size[2]\n",
    "\n",
    "HiddenSizeD = 768\n",
    "num_heads = 12\n",
    "TransformerLayer = 12\n",
    "\n",
    "MLP_size = 3072\n",
    "Transformer_MLP_unit = [MLP_size, HiddenSizeD]\n",
    "\n",
    "ClassHead_MLP_unit = [MLP_size]\n",
    "\n",
    "# 2.2. MLP\n",
    "dropout_rate = 0.1\n",
    "num_classes = 3\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 1\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1630633795713,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "r6eQhfUO906P",
    "outputId": "aa1906bc-1e6a-468a-f658-bbcb37d6c436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. featuremap Shape : (16, 16, 4, 1)\n",
      "2. patch_size : 1\n",
      "3. Number of Patches : 1024\n",
      "4. Dimension : 768\n",
      "5. Number of Heads : 12\n",
      "6. Number of Transformer Encedoer : 12\n",
      "7. MLP size : 3072\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. featuremap Shape : {featuremap_size[0], featuremap_size[1], featuremap_size[2], image_channel}\")\n",
    "print(f\"2. patch_size : {patch_size}\")\n",
    "print(f\"3. Number of Patches : {num_patches}\")\n",
    "print(f\"4. Dimension : {HiddenSizeD}\")\n",
    "print(f\"5. Number of Heads : {num_heads}\")\n",
    "print(f\"6. Number of Transformer Encedoer : {TransformerLayer}\")\n",
    "print(f\"7. MLP size : {MLP_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Standardization\n",
    "class WSConv3D(tf.keras.layers.Conv3D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(WSConv3D, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def standardize_weight(self, kernel, epsillon):\n",
    "        kernel_mean = tf.math.reduce_mean(kernel, axis=[0, 1, 2, 3], keepdims=True, name='kernel_mean')\n",
    "        kernel = kernel - kernel_mean\n",
    "        kernel_std = tf.keras.backend.std(kernel, axis=[0, 1, 2, 3], keepdims=True)\n",
    "        return kernel / (kernel_std + epsillon)\n",
    "\n",
    "    def call(self, inputs, epsillon=1e-4):\n",
    "        self.kernel.assign(self.standardize_weight(self.kernel, epsillon))\n",
    "        return super().call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1630633810784,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "pVl4_7Q1906Q"
   },
   "outputs": [],
   "source": [
    "# Patch creation as a layer\n",
    "class Patches(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches = num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, volume):\n",
    "        # volume (None, 16, 16, 4, 1024)\n",
    "        #print(f\"volume shape : {tf.shape(volume)}\")\n",
    "\n",
    "        batch_size = tf.shape(volume)[0]\n",
    "        #print(f\"type batch_size : {type(batch_size)}\")\n",
    "        patches_dim = volume.shape[-1]\n",
    "        #print(f\"type patches_dim : {type(patches_dim)}\")\n",
    "\n",
    "        patches = tf.reshape(volume, shape = [batch_size, self.num_patches, patches_dim])\n",
    "        #print(f\"patch shape 1D : {patches.shape}\") # (None, None, 1024)\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1630633817442,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "vqYlRAkD906Q"
   },
   "outputs": [],
   "source": [
    "# patch encoder\n",
    "class PatchEncoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    : Patch (driven from Linear Projection of Flattened Patches) + Position Embedding\n",
    "    Linear Projection\n",
    "    Position Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_patches = num_patches, \n",
    "                 HiddenSizeD = HiddenSizeD,\n",
    "                 ):\n",
    "        # num_patches = 16 * 16 * 4 = 1024\n",
    "        # HiddenSizeD = 768\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.HiddenSizeD = HiddenSizeD\n",
    "        self.projection = layers.Dense(units = HiddenSizeD)\n",
    "        \n",
    "        # Positional Embedding : Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "        self.pos_emb = self.add_weight(\"pos_emb\", shape=(1, num_patches, HiddenSizeD))\n",
    "        \n",
    "\n",
    "    def call(self, patch):\n",
    "        # preprojection patch : (1024, 1025)\n",
    "        patch = self.projection(patch)\n",
    "        # postprojection patch : (1024, 768)\n",
    "        return patch + self.pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1630633819509,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "ekE2L1uR906R"
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "\"\"\"\n",
    "GeLu : 0.5x(1 + tanh[p2/π(x + 0.044715x^3)])\n",
    "더 빠르게 수렴된단다.\n",
    "\"\"\"\n",
    "def mlp(x, units, dropout_rate):\n",
    "    for unit in units:\n",
    "        x = layers.Dense(unit, activation = tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1630633819867,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "3Oj2WpUw906S"
   },
   "outputs": [],
   "source": [
    "class Conv3DReLu(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, norm, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = \"same\"\n",
    "        self.strides = 1\n",
    "        self.norm = norm\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv = WSConv3D(\n",
    "            filters = self.filters, kernel_size = self.kernel_size, strides = self.strides,\n",
    "            padding = self.padding, use_bias = False, kernel_regularizer = tf.keras.regularizers.L2(1e-4), \n",
    "            kernel_initializer = \"lecun_normal\")\n",
    "\n",
    "        self.bn = BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "        self.gn = tfa.layers.GroupNormalization(epsilon=1e-5, groups = 16)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        if self.norm == \"bn\":\n",
    "            x = self.bn(x)\n",
    "        elif self.norm == \"gn\":\n",
    "            x = self.gn(x)\n",
    "        else:\n",
    "            raise Exception(\"bn or gn\")\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 871,
     "status": "ok",
     "timestamp": 1630634881761,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "6iyQTqb3906T"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, norm, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.norm = norm\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv1 = Conv3DReLu(filters = self.filters, kernel_size = 3, norm = self.norm)\n",
    "        self.conv2 = Conv3DReLu(filters = self.filters, kernel_size = 3, norm = self.norm)\n",
    "        self.upsampling = UpSampling3D(size = 2)\n",
    "\n",
    "    def call(self, inputs, skip = None):\n",
    "        x = self.upsampling(inputs)\n",
    "        if skip is not None:\n",
    "            x = tf.concat([x, skip], axis=-1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN3D(inputs, norm):\n",
    "    f1 = WSConv3D(filters = 64, kernel_size=3, activation=\"relu\", padding = \"same\")(inputs)\n",
    "    f1 = layers.MaxPool3D(pool_size = (2,2,2))(f1)\n",
    "    if norm == \"bn\":\n",
    "        f1 = layers.BatchNormalization()(f1) # 256\n",
    "    elif norm == \"gn\":\n",
    "        f1 = tfa.layers.GroupNormalization(epsilon=1e-5, groups = 16)(f1) # 256\n",
    "\n",
    "    f2 = WSConv3D(filters = 256, kernel_size=3, activation=\"relu\", padding = \"same\")(f1)\n",
    "    f2 = layers.MaxPool3D(pool_size = (2,2,2))(f2)\n",
    "    if norm == \"bn\":\n",
    "        f2 = layers.BatchNormalization()(f2) # 128\n",
    "    elif norm == \"gn\":\n",
    "        f2 = tfa.layers.GroupNormalization(epsilon=1e-5, groups = 16)(f2)\n",
    "\n",
    "    f3 = WSConv3D(filters = 512, kernel_size=3, activation=\"relu\", padding = \"same\")(f2)\n",
    "    f3 = layers.MaxPool3D(pool_size = (2,2,2))(f3)\n",
    "    if norm == \"bn\":\n",
    "        f3 = layers.BatchNormalization()(f3) # 64\n",
    "    elif norm == \"gn\":\n",
    "        f3 = tfa.layers.GroupNormalization(epsilon=1e-5, groups = 16)(f3)\n",
    "\n",
    "    f4 = WSConv3D(filters = 1024, kernel_size=3, activation=\"relu\", padding = \"same\")(f3)\n",
    "    f4 = layers.MaxPool3D(pool_size = (2,2,2))(f4)\n",
    "    if norm == \"bn\":\n",
    "        f4 = layers.BatchNormalization()(f4) # 32\n",
    "    elif norm == \"gn\":\n",
    "        f4 = tfa.layers.GroupNormalization(epsilon=1e-5, groups = 16)(f4)\n",
    "        \n",
    "    return [f4, f3, f2, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1630634882064,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "Quq3oIAd906T"
   },
   "outputs": [],
   "source": [
    "# Build the ViT model\n",
    "def create_model(norm,\n",
    "                 image_width = image_width,\n",
    "                 image_height= image_height,\n",
    "                 image_depth = image_depth,\n",
    "                 image_channel = image_channel,\n",
    "                 num_patches = num_patches,\n",
    "                 num_heads = num_heads,\n",
    "                 HiddenSizeD = HiddenSizeD,\n",
    "                 Transformer_MLP_unit = Transformer_MLP_unit,\n",
    "                 ClassHead_MLP_unit = ClassHead_MLP_unit\n",
    "                 ):\n",
    "    # 1. Encoder : (256, 256, 64, 1) -> (1024, 768)\n",
    "    # 1.1. CNN\n",
    "    # Input : (256, 256, 64, 1)\n",
    "    # output : (16, 16, 4, 512)\n",
    "    input_tensor = Input(shape=(image_width, image_height, image_depth, image_channel))\n",
    "    \n",
    "    \"\"\"\n",
    "    여기서 CNN Model이 들어가야한다.\n",
    "    \"\"\"\n",
    "    feature_maps = CNN3D(input_tensor, norm) # 32\n",
    "    \n",
    "    # (16, 16, 4, 1024), (32, 32, 8, 512), (64, 64, 16, 256), (128, 128, 32, 64)\n",
    "\n",
    "    # 1.2. Patch\n",
    "    # input : (16, 16, 4, 1024)\n",
    "    # output : (1024, 768)\n",
    "    patches = Patches()(feature_maps[0]) # (1024, 1024)\n",
    "    del feature_maps[0]\n",
    "    encoded_patches = PatchEncoder()(patches) # (1024, 768)\n",
    "    \n",
    "    \n",
    "    # 1.3. Transformer block.\n",
    "    # input : (1024, 768)\n",
    "    # output : (1024, 768)\n",
    "    for _ in range(TransformerLayer):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads, key_dim = HiddenSizeD, dropout = dropout_rate\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "       \n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, Transformer_MLP_unit, dropout_rate = dropout_rate)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "    \n",
    "    \n",
    "    # 2. Bridge : (1024, 768) -> (16, 16, 4, 768) -> (32, 32, 8, 512)\n",
    "    # 2.1. reshape\n",
    "    # Input : (1024, 768)\n",
    "    # output : (16, 16, 4, 768)\n",
    "    encodded_feature = tf.keras.layers.Reshape(target_shape = [image_width//16,\n",
    "                                                               image_height//16,\n",
    "                                                               image_depth//16,\n",
    "                                                               HiddenSizeD]\n",
    "                                               )(encoded_patches)\n",
    "    \n",
    "    # 2.2. Conv3D\n",
    "    # Input : (16, 16, 4, 768)\n",
    "    # output : (16, 16, 4, 512)\n",
    "    x = WSConv3D(filters = 512, kernel_size = 3, strides = 1, padding = \"same\",\n",
    "               use_bias=False, kernel_regularizer = tf.keras.regularizers.L2(1e-4), \n",
    "               kernel_initializer=\"lecun_normal\")(encodded_feature)\n",
    "    if norm == \"bn\":\n",
    "        x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    elif norm == \"gn\":\n",
    "        x = tfa.layers.GroupNormalization(epsilon=1e-5, groups = 16)(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # 3. Decoder CUP: (16, 16, 4, 512) -> (32, 32, 8, 256) -> (64, 64, 16, 128) -> (128, 128, 32, 64) -> (256, 256, 64, 16)\n",
    "    blocks = [DecoderBlock(filters = block_channel, norm = norm) \n",
    "              for block_channel in [256, 128, 64, 16]]\n",
    "    \n",
    "    for i, block in enumerate(blocks):\n",
    "        skip = feature_maps[i] if (i < 3) else None\n",
    "        x = block(x, skip = skip)\n",
    "    \n",
    "    \n",
    "    # 4. Segmentation Head\n",
    "    # Input : (256, 256, 64, 16)\n",
    "    # output : (256, 256, 64, 3)\n",
    "    x = WSConv3D(filters = num_classes, kernel_size = 1, padding = \"same\",\n",
    "                 name = \"segmentation_head_conv\"\n",
    "                )(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = input_tensor, outputs = x, name = \"TransUNet\")\n",
    "    return model\n",
    "    #\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4001,
     "status": "ok",
     "timestamp": 1630634886060,
     "user": {
      "displayName": "M Y",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiWGpD-tSy8n0gm6CCdo2duqvYpsc1vG1W6T2SY9g=s64",
      "userId": "04817009316790515734"
     },
     "user_tz": -540
    },
    "id": "ngpMIRit52lh",
    "outputId": "f7d588db-88a1-4a4c-b3b0-d24aae851042",
    "scrolled": false
   },
   "source": [
    "model = create_model(norm = \"bn\")\n",
    "model.summary(line_length = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8485d725"
   },
   "source": [
    "# 2. Dataset\n",
    "## 2.1. Kaggle : Body Morphometry: Kidney and Tumor\n",
    "####  https://www.kaggle.com/c/body-morphometry-kidney-and-tumor/discussion\n",
    "### -> To segment Kidney and Tumor on CT\n",
    "\n",
    "\n",
    "\n",
    "## 2.2. Data\n",
    " - Number of Data : 100 with 64 slices \n",
    " - Size : (512, 512, 64)\n",
    " - HU : -1024 ~ 3096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5LwXWwWd906V",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_x_dir = f\"{base_dir}/train/DICOM/\"\n",
    "train_y_dir = f\"{base_dir}/train/Label/\"\n",
    "train_x_folder_list = os.listdir(train_x_dir)\n",
    "#print(train_x_folder_list)\n",
    "\n",
    "train_y_folder_list = os.listdir(train_y_dir)\n",
    "#print(train_y_folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "190c4640"
   },
   "outputs": [],
   "source": [
    "def plot_slices(data, vmin = None, vmax = None, cmap = plt.cm.bone, dcm_flag = 0):\n",
    "    f, axarr = plt.subplots(8, 8, figsize=(50, 50))\n",
    "    cnt = 0\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if dcm_flag == 1:\n",
    "                axarr[i, j].imshow(data[:, :, cnt], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            else:\n",
    "                axarr[i, j].imshow(data[:, :, cnt, :], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            axarr[i, j].set_title(f'slice {cnt+1}', fontsize=20)\n",
    "            axarr[i, j].axis(\"off\")\n",
    "            cnt +=1\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f7b10e0",
    "scrolled": true
   },
   "source": [
    "for i, folder in enumerate(train_x_folder_list):\n",
    "    # 열어서 파일 list 만들고\n",
    "    files_list = os.listdir(f\"{train_x_dir}{folder}/\")\n",
    "    images = np.zeros([512, 512, 64])\n",
    "    \n",
    "    for j, file in enumerate(files_list):\n",
    "        # 각 파일은 pydicom으로 열기\n",
    "        image_dir = f\"{train_x_dir}{folder}/{file}\"\n",
    "        image = dcm.read_file(image_dir).pixel_array\n",
    "        images[:, :, j] = image\n",
    "\n",
    "    plot_slices(images, dcm_flag = 1)\n",
    "\n",
    "    if i== 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd7a157b",
    "scrolled": true
   },
   "source": [
    "for i, folder in enumerate(train_y_folder_list):\n",
    "    # 열어서 파일 list 만들고\n",
    "    files_list = os.listdir(f\"{train_y_dir}{folder}/\")\n",
    "    labels = np.zeros([512, 512, 64])\n",
    "    \n",
    "    for j, file in enumerate(files_list):\n",
    "        # 각 파일은 pydicom으로 열기\n",
    "        label_dir = f\"{train_y_dir}{folder}/{file}\"\n",
    "        label = cv2.imread(label_dir, 0)\n",
    "        labels[:, :, j] = label\n",
    "    \n",
    "    plot_slices(labels, 0, 2, cmap = None, dcm_flag = 1)\n",
    "    if i== 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3813cada"
   },
   "source": [
    "## 2.3. Preprocessing\n",
    " - Image Scaling : HU to Grayscale\n",
    " - Data Augmentation : 최대한 많이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4c4d701"
   },
   "source": [
    "### 2.3.1. Image Scaling and Save\n",
    "#### Window Level : 30\n",
    "#### Window Width : 400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.2.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL\n",
    "PIL.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.18.5'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "db0d4e86",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__array__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-310b99dcfeaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_channel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __array__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "wl, ww = 30, 400\n",
    "\n",
    "\n",
    "for i, folder in enumerate(train_y_folder_list):\n",
    "    # 열어서 파일 list 만들고\n",
    "\n",
    "    files_list = os.listdir(f\"{train_y_dir}{folder}/\")\n",
    "\n",
    "    #masks = np.zeros([image_height, image_width, image_depth, num_classes], dtype = np.float32)\n",
    " \n",
    "    masks = np.zeros([image_height, image_width, image_depth, image_channel], dtype = np.float32)\n",
    "   \n",
    "    volume = np.zeros([image_height, image_width, image_depth, image_channel], dtype = np.uint8)\n",
    "    \n",
    "    for j, file in enumerate(files_list):\n",
    "        file_name, file_format = file.split(\".\")\n",
    "        label_dir = f\"{train_y_dir}{folder}/{file_name}.png\"\n",
    "        label = cv2.imread(label_dir, 0)\n",
    "        label = cv2.resize(label, dsize=(image_width, image_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # 1. label\n",
    "        label[label < 0.5] = 0\n",
    "        label[label >= 1.5] = 2\n",
    "        label[(label >= 0.5) & (label < 1.5)] = 1\n",
    "        \"\"\"\n",
    "        label = tf.one_hot(label,\n",
    "                           depth=num_classes,\n",
    "                           dtype=tf.uint8,\n",
    "                          )\n",
    "    \n",
    "        label = tf.reshape(label, [image_height, image_width, num_classes])\n",
    "        \"\"\"\n",
    "\n",
    "        label = tf.reshape(label, [image_height, image_width, image_channel])\n",
    "        masks[:,:,j,:] = label\n",
    "        print(3)\n",
    "\n",
    "        # 2. image\n",
    "        image_dir = f\"body-morphometry-kidney-and-tumor/train/DICOM/{folder}/{file_name}.dcm\"\n",
    "        raw_image = dcm.read_file(image_dir).pixel_array\n",
    "        raw_image = cv2.resize(raw_image, dsize=(image_width, image_height), interpolation=cv2.INTER_AREA)\n",
    "        image = np.clip(raw_image, wl - (ww / 2), wl + (ww / 2))\n",
    "        max_v, min_v = image.max(), image.min()\n",
    "        image = (image - min_v) / (max_v - min_v) * 255\n",
    "        image = np.reshape(image , [image_height, image_width, image_channel])\n",
    "        volume[:,:,j,:] = image\n",
    "\n",
    "    np.save(f\"C:/Users/user/Desktop/body-morphometry-kidney-and-tumor/train/Data_3/patient_{i+1}\", volume)\n",
    "    np.save(f\"C:/Users/user/Desktop/body-morphometry-kidney-and-tumor/train/Mask_3/patient_{i+1}\", masks)\n",
    "    print(f\"Patient{i+1} complete\")\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- -------------------\n",
      "-illow                             7.2.0\n",
      "-umpy                              1.18.5\n",
      "absl-py                            0.15.0\n",
      "alabaster                          0.7.12\n",
      "anaconda-client                    1.7.2\n",
      "anaconda-navigator                 1.9.12Note: you may need to restart the kernel to use updated packages.\n",
      "anaconda-project                   0.8.3\n",
      "argh                               0.26.2\n",
      "asn1crypto                         1.3.0\n",
      "astroid                            2.4.2\n",
      "astropy                            4.0.1.post1\n",
      "astunparse                         1.6.3\n",
      "atomicwrites                       1.4.0\n",
      "attrs                              19.3.0\n",
      "autopep8                           1.5.3\n",
      "Babel                              2.8.0\n",
      "backcall                           0.2.0\n",
      "backports.functools-lru-cache      1.6.1\n",
      "backports.shutil-get-terminal-size 1.0.0\n",
      "backports.tempfile                 1.0\n",
      "backports.weakref                  1.0.post1\n",
      "bcrypt                             3.1.7\n",
      "beautifulsoup4                     4.9.1\n",
      "bitarray                           1.4.0\n",
      "bkcharts                           0.2\n",
      "\n",
      "bleach                             3.1.5\n",
      "bokeh                              2.1.1\n",
      "boto                               2.49.0\n",
      "Bottleneck                         1.3.2\n",
      "brotlipy                           0.7.0\n",
      "cachetools                         5.0.0\n",
      "certifi                            2020.6.20\n",
      "cffi                               1.14.0\n",
      "chardet                            3.0.4\n",
      "click                              7.1.2\n",
      "cloudpickle                        1.5.0\n",
      "clyent                             1.2.2\n",
      "colorama                           0.4.3\n",
      "comtypes                           1.1.7\n",
      "conda                              4.8.3\n",
      "conda-build                        3.18.11\n",
      "conda-package-handling             1.7.0\n",
      "conda-verify                       3.4.2\n",
      "contextlib2                        0.6.0.post1\n",
      "cryptography                       2.9.2\n",
      "cycler                             0.10.0\n",
      "Cython                             0.29.21\n",
      "cytoolz                            0.10.1\n",
      "dask                               2.20.0\n",
      "decorator                          4.4.2\n",
      "defusedxml                         0.6.0\n",
      "diff-match-patch                   20200713\n",
      "distributed                        2.20.0\n",
      "docutils                           0.16\n",
      "entrypoints                        0.3\n",
      "et-xmlfile                         1.0.1\n",
      "fastcache                          1.1.0\n",
      "filelock                           3.0.12\n",
      "flake8                             3.8.3\n",
      "Flask                              1.1.2\n",
      "flatbuffers                        1.12\n",
      "fsspec                             0.7.4\n",
      "future                             0.18.2\n",
      "gast                               0.3.3\n",
      "gevent                             20.6.2\n",
      "glob2                              0.7\n",
      "gmpy2                              2.0.8\n",
      "google-auth                        2.6.5\n",
      "google-auth-oauthlib               0.4.6\n",
      "google-pasta                       0.2.0\n",
      "greenlet                           0.4.16\n",
      "grpcio                             1.32.0\n",
      "h5py                               2.10.0\n",
      "HeapDict                           1.0.1\n",
      "html5lib                           1.1\n",
      "idna                               2.10\n",
      "imageio                            2.9.0\n",
      "imagesize                          1.2.0\n",
      "imgaug                             0.4.0\n",
      "importlib-metadata                 4.11.3\n",
      "intervaltree                       3.0.2\n",
      "ipykernel                          5.3.2\n",
      "ipython                            7.16.1\n",
      "ipython-genutils                   0.2.0\n",
      "ipywidgets                         7.5.1\n",
      "isort                              4.3.21\n",
      "itsdangerous                       1.1.0\n",
      "jdcal                              1.4.1\n",
      "jedi                               0.17.1\n",
      "Jinja2                             2.11.2\n",
      "joblib                             0.16.0\n",
      "json5                              0.9.5\n",
      "jsonschema                         3.2.0\n",
      "jupyter                            1.0.0\n",
      "jupyter-client                     6.1.6\n",
      "jupyter-console                    6.1.0\n",
      "jupyter-core                       4.6.3\n",
      "jupyterlab                         2.1.5\n",
      "jupyterlab-server                  1.2.0\n",
      "keras                              2.7.0\n",
      "Keras-Preprocessing                1.1.2\n",
      "keyring                            21.2.1\n",
      "kiwisolver                         1.2.0\n",
      "lazy-object-proxy                  1.4.3\n",
      "libarchive-c                       2.9\n",
      "libclang                           13.0.0\n",
      "llvmlite                           0.33.0+1.g022ab0f\n",
      "locket                             0.2.0\n",
      "lxml                               4.5.2\n",
      "Markdown                           3.3.6\n",
      "MarkupSafe                         1.1.1\n",
      "matplotlib                         3.2.2\n",
      "mccabe                             0.6.1\n",
      "menuinst                           1.4.16\n",
      "mistune                            0.8.4\n",
      "mkl-fft                            1.1.0\n",
      "mkl-random                         1.1.1\n",
      "mkl-service                        2.3.0\n",
      "mock                               4.0.2\n",
      "more-itertools                     8.4.0\n",
      "mpmath                             1.1.0\n",
      "msgpack                            1.0.0\n",
      "multipledispatch                   0.6.0\n",
      "navigator-updater                  0.2.1\n",
      "nbconvert                          5.6.1\n",
      "nbformat                           5.0.7\n",
      "networkx                           2.4\n",
      "nltk                               3.5\n",
      "nose                               1.3.7\n",
      "notebook                           6.0.3\n",
      "numba                              0.50.1\n",
      "numexpr                            2.7.1\n",
      "numpy                              1.19.5\n",
      "numpydoc                           1.1.0\n",
      "oauthlib                           3.2.0\n",
      "olefile                            0.46\n",
      "opencv-python                      4.5.5.64\n",
      "openpyxl                           3.0.4\n",
      "opt-einsum                         3.3.0\n",
      "packaging                          20.4\n",
      "pandas                             1.0.5\n",
      "pandocfilters                      1.4.2\n",
      "paramiko                           2.7.1\n",
      "parso                              0.7.0\n",
      "partd                              1.1.0\n",
      "path                               13.1.0\n",
      "pathlib2                           2.3.5\n",
      "pathtools                          0.1.2\n",
      "patsy                              0.5.1\n",
      "pep8                               1.7.1\n",
      "pexpect                            4.8.0\n",
      "pickleshare                        0.7.5\n",
      "Pillow                             8.2.0\n",
      "pip                                20.1.1\n",
      "pkginfo                            1.5.0.1\n",
      "pluggy                             0.13.1\n",
      "ply                                3.11\n",
      "prometheus-client                  0.8.0\n",
      "prompt-toolkit                     3.0.5\n",
      "protobuf                           3.20.0\n",
      "psutil                             5.7.0\n",
      "py                                 1.9.0\n",
      "pyasn1                             0.4.8\n",
      "pyasn1-modules                     0.2.8\n",
      "pycodestyle                        2.6.0\n",
      "pycosat                            0.6.3\n",
      "pycparser                          2.20\n",
      "pycurl                             7.43.0.5\n",
      "pydicom                            2.3.0\n",
      "pydocstyle                         5.0.2\n",
      "pyflakes                           2.2.0\n",
      "Pygments                           2.6.1\n",
      "pylint                             2.5.3\n",
      "PyNaCl                             1.4.0\n",
      "pyodbc                             4.0.0-unsupported\n",
      "pyOpenSSL                          19.1.0\n",
      "pyparsing                          2.4.7\n",
      "pyreadline                         2.1\n",
      "pyrsistent                         0.16.0\n",
      "PySocks                            1.7.1\n",
      "pytest                             5.4.3\n",
      "python-dateutil                    2.8.1\n",
      "python-jsonrpc-server              0.3.4\n",
      "python-language-server             0.34.1\n",
      "pytz                               2020.1\n",
      "PyWavelets                         1.1.1\n",
      "pywin32                            227\n",
      "pywin32-ctypes                     0.2.0\n",
      "pywinpty                           0.5.7\n",
      "PyYAML                             5.3.1\n",
      "pyzmq                              19.0.1\n",
      "QDarkStyle                         2.8.1\n",
      "QtAwesome                          0.7.2\n",
      "qtconsole                          4.7.5\n",
      "QtPy                               1.9.0\n",
      "regex                              2020.6.8\n",
      "requests                           2.24.0\n",
      "requests-oauthlib                  1.3.1\n",
      "rope                               0.17.0\n",
      "rsa                                4.8\n",
      "Rtree                              0.9.4\n",
      "ruamel-yaml                        0.15.87\n",
      "scikit-image                       0.16.2\n",
      "scikit-learn                       0.23.1\n",
      "scipy                              1.5.0\n",
      "seaborn                            0.10.1\n",
      "Send2Trash                         1.5.0\n",
      "setuptools                         49.2.0.post20200714\n",
      "Shapely                            1.8.1.post1\n",
      "simplegeneric                      0.8.1\n",
      "singledispatch                     3.4.0.3\n",
      "sip                                4.19.13\n",
      "six                                1.15.0\n",
      "snowballstemmer                    2.0.0\n",
      "sortedcollections                  1.2.1\n",
      "sortedcontainers                   2.2.2\n",
      "soupsieve                          2.0.1\n",
      "Sphinx                             3.1.2\n",
      "sphinxcontrib-applehelp            1.0.2\n",
      "sphinxcontrib-devhelp              1.0.2\n",
      "sphinxcontrib-htmlhelp             1.0.3\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               1.0.3\n",
      "sphinxcontrib-serializinghtml      1.1.4\n",
      "sphinxcontrib-websupport           1.2.3\n",
      "spyder                             4.1.4\n",
      "spyder-kernels                     1.9.2\n",
      "SQLAlchemy                         1.3.18\n",
      "statsmodels                        0.11.1\n",
      "sympy                              1.6.1\n",
      "tables                             3.6.1\n",
      "tblib                              1.6.0\n",
      "tensorboard                        2.8.0\n",
      "tensorboard-data-server            0.6.1\n",
      "tensorboard-plugin-wit             1.8.1\n",
      "tensorflow                         2.4.1\n",
      "tensorflow-addons                  0.16.1\n",
      "tensorflow-estimator               2.4.0\n",
      "tensorflow-io-gcs-filesystem       0.24.0\n",
      "termcolor                          1.1.0\n",
      "terminado                          0.8.3\n",
      "testpath                           0.4.4\n",
      "threadpoolctl                      2.1.0\n",
      "toml                               0.10.1\n",
      "toolz                              0.10.0\n",
      "tornado                            6.0.4\n",
      "tqdm                               4.47.0\n",
      "traitlets                          4.3.3\n",
      "typeguard                          2.13.3\n",
      "typing-extensions                  3.7.4.2\n",
      "ujson                              1.35\n",
      "unicodecsv                         0.14.1\n",
      "urllib3                            1.25.9\n",
      "watchdog                           0.10.3\n",
      "wcwidth                            0.2.5\n",
      "webencodings                       0.5.1\n",
      "Werkzeug                           1.0.1\n",
      "wheel                              0.37.1\n",
      "widgetsnbextension                 3.5.1\n",
      "win-inet-pton                      1.1.0\n",
      "win-unicode-console                0.5\n",
      "wincertstore                       0.2\n",
      "wrapt                              1.12.1\n",
      "xlrd                               1.2.0\n",
      "XlsxWriter                         1.2.9\n",
      "xlwings                            0.19.5\n",
      "xlwt                               1.3.0\n",
      "xmltodict                          0.12.0\n",
      "yapf                               0.30.0\n",
      "zict                               2.0.0\n",
      "zipp                               3.1.0\n",
      "zope.event                         4.4\n",
      "zope.interface                     4.7.1\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3b9091a"
   },
   "source": [
    "### 2.3.2. Data Augmentation\n",
    "\n",
    "하는 방법 찾기\n",
    "\n",
    "- Flip (Vertical, Horizontal)\n",
    "\n",
    "- Random Rotation -170 ~ +170 (10)\n",
    "\n",
    "- Translate\n",
    "\n",
    "- Gaussian Noise\n",
    "\n",
    "- Random Shear\n",
    "\n",
    "- Gamma Contrast\n",
    "\n",
    "- Gaussian_blur\n",
    "\n",
    "--> x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imgaug\n",
      "  Using cached imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from imgaug) (8.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from imgaug) (1.15.0)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imgaug) (0.16.2)\n",
      "Collecting Shapely\n",
      "  Using cached Shapely-1.8.1.post1-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (from imgaug) (3.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from imgaug) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imgaug) (1.19.5)\n",
      "Requirement already satisfied: imageio in c:\\users\\user\\anaconda3\\lib\\site-packages (from imgaug) (2.9.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\user\\anaconda3\\lib\\site-packages (from imgaug) (4.5.5.64)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (2.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug) (4.4.2)\n",
      "Installing collected packages: Shapely, imgaug\n",
      "Successfully installed Shapely-1.8.1.post1 imgaug-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imgaug\n",
    "import imgaug.augmenters as iaa\n",
    "import skimage as sk\n",
    "import cv2\n",
    "\n",
    "# 1. Flip\n",
    "def Flip(data, mask, flag,\n",
    "         width = image_width,\n",
    "         height = image_height,\n",
    "         depth = image_depth,\n",
    "         channel = image_channel,\n",
    "         num_classes = num_classes\n",
    "        ):\n",
    "    aug_data = np.zeros([width, height, depth, channel], dtype = np.float32)\n",
    "    aug_mask = np.zeros([width, height, depth, channel], dtype = np.float32)\n",
    "    if flag == \"lr\":\n",
    "        for i in range(depth):\n",
    "            aug_data[:, :, i, :] = np.reshape(cv2.flip(data[:, :, i, :], 1), [width, height, channel])\n",
    "            aug_mask[:, :, i, :] = np.reshape(cv2.flip(mask[:, :, i, :], 1), [width, height, channel])\n",
    "    else:\n",
    "        for i in range(depth):\n",
    "            aug_data[:, :, i, :] = np.reshape(cv2.flip(data[:, :, i, :], 0), [width, height, channel])\n",
    "            aug_mask[:, :, i, :] = np.reshape(cv2.flip(mask[:, :, i, :], 0), [width, height, channel])\n",
    "    \n",
    "    aug_mask = np.eye(num_classes)[aug_mask.astype(int)].astype('float32')\n",
    "    aug_mask = np.reshape(aug_mask, [height, width, depth, num_classes])\n",
    "    return aug_data, aug_mask\n",
    "\n",
    "    \n",
    "\n",
    "# 2. Rotation (-170 ~ +170)\n",
    "def Rotation(data, mask,\n",
    "             width = image_width,\n",
    "             height = image_height,\n",
    "             depth = image_depth,\n",
    "             channel = image_channel,\n",
    "             num_classes = num_classes\n",
    "            ):\n",
    "    \n",
    "    aug_data = np.zeros([width, height, depth, channel], dtype = np.float32)\n",
    "    aug_mask = np.zeros([width, height, depth, channel], dtype = np.float32)\n",
    "    \n",
    "    rot_matrix = cv2.getRotationMatrix2D((int(width/2), int(height/2)), int(random.uniform(-17, 17)) *10, 1)\n",
    "    \n",
    "    for i in range(depth):\n",
    "        aug_data[: ,:, i, :] = np.reshape(cv2.warpAffine(data[: ,:, i, :], rot_matrix, (width, height)), [width, height, channel])\n",
    "        aug_mask[: ,:, i, :] = np.reshape(cv2.warpAffine(mask[: ,:, i, :], rot_matrix, (width, height)), [width, height, channel])\n",
    "    \n",
    "    aug_mask[aug_mask < 0.5] = 0\n",
    "    aug_mask[aug_mask >= 1.5] = 2\n",
    "    aug_mask[(aug_mask >= 0.5) & (aug_mask < 1.5)] = 1\n",
    "    \n",
    "    aug_mask = np.eye(num_classes)[aug_mask.astype(int)].astype('float32')\n",
    "    aug_mask = np.reshape(aug_mask, [height, width, depth, num_classes])\n",
    "    return aug_data, aug_mask\n",
    "\n",
    "\n",
    "# 3. Translate\n",
    "def Translate(data, mask,\n",
    "              width = image_width,\n",
    "              height = image_height,\n",
    "              depth = image_depth,\n",
    "              channel = image_channel,\n",
    "              num_classes = num_classes\n",
    "             ):\n",
    "    aug_data = np.zeros([width, height, depth, channel], dtype = np.float32)\n",
    "    aug_mask = np.zeros([width, height, depth, channel], dtype = np.float32)\n",
    "    \n",
    "    x_shift = random.uniform(-width//2, width//2)\n",
    "    y_shift = random.uniform(-height//2, height//2)\n",
    "    \n",
    "    shift_matrix = np.float32([[1, 0, x_shift],\n",
    "                               [0, 1, y_shift]])  \n",
    "    \n",
    "    for i in range(depth):\n",
    "        aug_data[:, :, i, :] = np.reshape(cv2.warpAffine(data[:, :, i, :], shift_matrix, (width, height)), [width, height, channel])\n",
    "        aug_mask[:, :, i, :] = np.reshape(cv2.warpAffine(mask[:, :, i, :], shift_matrix, (width, height)), [width, height, channel])\n",
    "    \n",
    "    aug_mask[aug_mask < 0.5] = 0\n",
    "    aug_mask[aug_mask >= 1.5] = 2\n",
    "    aug_mask[(aug_mask >= 0.5) & (aug_mask < 1.5)] = 1\n",
    "    \n",
    "    aug_mask = np.eye(num_classes)[aug_mask.astype(int)].astype('float32')\n",
    "    aug_mask = np.reshape(aug_mask, [height, width, depth, num_classes])\n",
    "    return aug_data, aug_mask\n",
    "\n",
    "\n",
    "# 4. Random Shear\n",
    "def Shear(data, mask,\n",
    "          width = image_width,\n",
    "          height = image_height,\n",
    "          depth = image_depth,\n",
    "          channel = image_channel,\n",
    "          num_classes = num_classes\n",
    "         ):\n",
    "    rand_deg = np.random.randint(-10, 10)\n",
    "    shear = iaa.Affine(shear=rand_deg)\n",
    "    aug_data = shear.augment_images(data)\n",
    "    aug_mask = shear.augment_images(mask)\n",
    "    \n",
    "    aug_mask[aug_mask < 0.5] = 0\n",
    "    aug_mask[aug_mask >= 1.5] = 2\n",
    "    aug_mask[(aug_mask >= 0.5) & (aug_mask < 1.5)] = 1\n",
    "    \n",
    "    aug_mask = np.eye(num_classes)[aug_mask.astype(int)].astype('float32')\n",
    "    aug_mask = np.reshape(aug_mask, [height, width, depth, num_classes])\n",
    "    return aug_data, aug_mask\n",
    "\n",
    "#################################################################################\n",
    "# 5. Gaussian Noise\n",
    "def GaussianNoise(data, mask,\n",
    "                  width = image_width,\n",
    "                  height = image_height,\n",
    "                  depth = image_depth,\n",
    "                  channel = image_channel,\n",
    "                  num_classes = num_classes\n",
    "                 ):\n",
    "    #data = data/ 255.\n",
    "    aug_data = sk.util.random_noise(data, mode='gaussian')\n",
    "    \n",
    "    #aug_mask = np.eye(num_classes)[mask.astype(int)].astype('float32')\n",
    "    #aug_mask = np.reshape(aug_mask, [height, width, depth, num_classes])\n",
    "    return aug_data, mask\n",
    "\n",
    "\n",
    "# 6. Gamma Contrast\n",
    "def GammaContrast(data, mask,\n",
    "                  min_range=0.5, max_range=2.0,\n",
    "                  width = image_width,\n",
    "                  height = image_height,\n",
    "                  depth = image_depth,\n",
    "                  channel = image_channel,\n",
    "                  num_classes = num_classes\n",
    "                 ):\n",
    "    rand_gamma = np.random.uniform(min_range, max_range)\n",
    "    gamma = iaa.Sequential(iaa.GammaContrast(rand_gamma))\n",
    "    aug_data = gamma.augment_images(data)\n",
    "    \n",
    "    #aug_mask = np.eye(num_classes)[mask.astype(int)].astype('float32')\n",
    "    #aug_mask = np.reshape(aug_mask, [height, width, depth, num_classes])\n",
    "    return aug_data, mask\n",
    "\n",
    "# 7. Gaussian_blur\n",
    "def GaussianBlur(data, mask,\n",
    "                 width = image_width,\n",
    "                 height = image_height,\n",
    "                 depth = image_depth,\n",
    "                 channel = image_channel,\n",
    "                 num_classes = num_classes\n",
    "                ):\n",
    "    rand_sigma = np.random.uniform(0.0, 3.0)\n",
    "    blur = iaa.GaussianBlur(sigma=rand_sigma)\n",
    "    aug_data = blur.augment_images(data)\n",
    "    \n",
    "    #aug_mask = np.eye(num_classes)[mask.astype(int)].astype('float32')\n",
    "    #aug_mask = np.reshape(aug_mask, [height, width, depth, num_classes])\n",
    "    return aug_data, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/user/Desktop/body-morphometry-kidney-and-tumor/train/Data_3/patient_1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1bd3e427ec21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m101\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{save_dir}Data_3/patient_{i}.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{save_dir}Data_3/patient_{i}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/user/Desktop/body-morphometry-kidney-and-tumor/train/Data_3/patient_1.npy'"
     ]
    }
   ],
   "source": [
    "for i in range(1, 101):\n",
    "    src = np.load(f\"{save_dir}Data_3/patient_{i}.npy\")\n",
    "    src = src / 255.\n",
    "    np.save(f\"{save_dir}Data_3/patient_{i}\", src)\n",
    "    \n",
    "    target = np.load(f\"{save_dir}Mask_3/patient_{i}.npy\")\n",
    "    \n",
    "    # 1. Left-Right Flip\n",
    "    tmp_src, tmp_target = Flip(src, target, 'lr')\n",
    "    np.save(f\"{save_dir}Data_3/patient_{i}_lrflip\", tmp_src)\n",
    "    np.save(f\"{save_dir}Mask_3/patient_{i}_lrflip\", tmp_target)\n",
    "    #print(f\"{i} - LR flip : {tmp_target.shape}\")\n",
    "    \n",
    "    # 2. Up-Down Flip\n",
    "    tmp_src, tmp_target = Flip(src, target, 'ud')\n",
    "    np.save(f\"{save_dir}Data_3/patient_{i}_udflip\", tmp_src)\n",
    "    np.save(f\"{save_dir}Mask_3/patient_{i}_udflip\", tmp_target)\n",
    "    #print(f\"{i} - UD flip : {tmp_target.shape}\")\n",
    "    \n",
    "    # 3. Rotation\n",
    "    tmp_src, tmp_target = Rotation(src, target)\n",
    "    np.save(f\"{save_dir}Data_3/patient_{i}_rot\", tmp_src)\n",
    "    np.save(f\"{save_dir}Mask_3/patient_{i}_rot\", tmp_target)\n",
    "    #print(f\"{i} - Rotation flip : {tmp_target.shape}\")\n",
    "    \n",
    "    # 4. Translate\n",
    "    tmp_src, tmp_target = Translate(src, target)\n",
    "    np.save(f\"{save_dir}Data_3/patient_{i}_translate\", tmp_src)\n",
    "    np.save(f\"{save_dir}Mask_3/patient_{i}_translate\", tmp_target)\n",
    "    #print(f\"{i} - Translate flip : {tmp_target.shape}\")\n",
    "    \n",
    "    # 5.Random Shear\n",
    "    tmp_src, tmp_target = Shear(src, target)\n",
    "    np.save(f\"{save_dir}Data_3/patient_{i}_shear\", tmp_src)\n",
    "    np.save(f\"{save_dir}Mask_3/patient_{i}_shear\", tmp_target)\n",
    "    #print(f\"{i} - Shear flip : {tmp_target.shape}\")\n",
    "    \n",
    "    #############################################################\n",
    "    target = np.eye(num_classes)[target.astype(int)].astype('float32')\n",
    "    target = np.reshape(target, [image_height, image_width, image_depth, num_classes])\n",
    "    np.save(f\"{save_dir}Mask_3/patient_{i}\", target)\n",
    "    \n",
    "    # 6. Gaussian Noise\n",
    "    tmp_src, _ = GaussianNoise(src, target)\n",
    "    np.save(f\"{save_dir}Data_3/patient_{i}_noise\", tmp_src)\n",
    "    np.save(f\"{save_dir}Mask_3/patient_{i}_noise\", target) \n",
    "    \n",
    "    # 7. Gamma Contrast\n",
    "    tmp_src, _ = GammaContrast(src, target, min_range = 0.99, max_range = 1.01) # 3\n",
    "    np.save(f\"{save_dir}Data_3/patient_{i}_gamma\", tmp_src)\n",
    "    np.save(f\"{save_dir}Mask_3/patient_{i}_gamma\", target)\n",
    "    \n",
    "    # 8. Gaussian_blur\n",
    "    tmp_src, _ = GaussianBlur(src, target)\n",
    "    np.save(f\"{save_dir}Data_3/patient_{i}_blur\", tmp_src)\n",
    "    np.save(f\"{save_dir}Mask_3/patient_{i}_blur\", target)\n",
    "    print(f\"Patient{i} complete\")\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.load(f\"{save_dir}Data_3/patient_1.npy\")\n",
    "plot_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask = np.load(f\"{save_dir}Mask_3/patient_1.npy\")\n",
    "print(mask.shape)\n",
    "#plot_slices(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_nWB7nlzhu_",
    "scrolled": false
   },
   "source": [
    "data = np.load(f\"{save_dir}Data_3/patient_1_udflip.npy\")\n",
    "plot_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.load(f\"{save_dir}Data_3/patient_1_lrflip.npy\")\n",
    "plot_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.load(f\"{save_dir}Data_3/patient_1_rot.npy\")\n",
    "plot_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data = np.load(f\"{save_dir}Data_3/patient_1_translate.npy\")\n",
    "plot_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask = np.load(f\"{save_dir}Mask_3/patient_1_translate.npy\")\n",
    "print(mask.shape)\n",
    "#plot_slices(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.load(f\"{save_dir}Data_3/patient_1_shear.npy\")\n",
    "plot_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data = np.load(f\"{save_dir}Data_3/patient_1_noise.npy\")\n",
    "plot_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.load(f\"{save_dir}Data_3/patient_1_gamma.npy\")\n",
    "plot_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.load(f\"{save_dir}Data_3/patient_1_blur.npy\")\n",
    "plot_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99025f93"
   },
   "source": [
    "### Dataset Train : Test : Valid = 8 : 1 : 1\n",
    "\n",
    "### By Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702 801\n",
      "Number of Trainset : 702\n",
      "Number of Validnset : 99\n",
      "Number of Testset : 99\n",
      "Total : 900\n"
     ]
    }
   ],
   "source": [
    "name_list = os.listdir(f\"{base_dir}/train/Data_3/\")\n",
    "#print(name_list)\n",
    "print(name_list.index(\"patient_8.npy\"), name_list.index(\"patient_9.npy\"))\n",
    "train_name = name_list[:name_list.index(\"patient_8.npy\")]\n",
    "valid_name = name_list[name_list.index(\"patient_8.npy\") : name_list.index(\"patient_9.npy\")]\n",
    "test_name = name_list[name_list.index(\"patient_9.npy\") :]\n",
    "print(f\"Number of Trainset : {len(train_name)}\")\n",
    "print(f\"Number of Validnset : {len(valid_name)}\")\n",
    "print(f\"Number of Testset : {len(test_name)}\")\n",
    "print(f\"Total : {len(train_name) + len(valid_name) + len(test_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "49599810"
   },
   "outputs": [],
   "source": [
    "def train_generator(names = train_name):\n",
    "    for name in names:\n",
    "        x_dir = f\"{base_dir}/train/Data_3/{name}\"\n",
    "        y_dir = f\"{base_dir}/train/Mask_3/{name}\"\n",
    "\n",
    "        x = np.load(x_dir)\n",
    "        y = np.load(y_dir)\n",
    "        yield (x, y)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(train_generator,\n",
    "                                               (tf.float32, tf.float32),\n",
    "                                               (tf.TensorShape([image_height, image_width, image_depth, image_channel]), tf.TensorShape([image_height, image_width, image_depth, num_classes])),\n",
    "                                              )\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "bd381fac"
   },
   "outputs": [],
   "source": [
    "def valid_generator(names = valid_name):\n",
    "    for name in names:\n",
    "        x_dir = f\"{base_dir}/train/Data_3/{name}\"\n",
    "        y_dir = f\"{base_dir}/train/Mask_3/{name}\"\n",
    "\n",
    "        x = np.load(x_dir)\n",
    "        y = np.load(y_dir)\n",
    "        yield (x, y)\n",
    "        \n",
    "valid_dataset = tf.data.Dataset.from_generator(valid_generator,\n",
    "                                               (tf.float32, tf.float32),\n",
    "                                               (tf.TensorShape([image_height, image_width, image_depth, image_channel]), tf.TensorShape([image_height, image_width, image_depth, num_classes])),\n",
    "                                              )\n",
    "valid_dataset = valid_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb10b9ed"
   },
   "source": [
    "# 3. Training\n",
    "### Model : TransUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dice_per_class(y_true, y_pred):\n",
    "    smooth = 0.0001\n",
    "    #y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    return (2 * tf.reduce_sum(y_true * y_pred) + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "c80e2521"
   },
   "outputs": [],
   "source": [
    "def Dice(y_true, y_pred, flag = 0, num_classes = num_classes):\n",
    "    y_pred = tf.nn.softmax(y_pred)\n",
    "    dice = 0.0\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        if flag == 0:\n",
    "            # (None, 256, 256, 64, 3)\n",
    "            dice += Dice_per_class(y_true[:, :, :, :, class_idx], y_pred[:, :, :, :, class_idx])\n",
    "        else:\n",
    "            dice += Dice_per_class(y_true[:, :, :, class_idx], y_pred[:, :, :, class_idx])\n",
    "    return dice / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "1507f2e3"
   },
   "outputs": [],
   "source": [
    "class DiceLoss(tf.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__(reduction = 'auto', name = \"DiceLoss\")\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        dice = Dice(y_true, y_pred)\n",
    "        return 1-dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class FocalTverskyLoss(tf.losses.Loss):\n",
    "    def __init__(self, alpha = 0.7, gamma = 1, smooth = 1e-6):\n",
    "        super(FocalTverskyLoss, self).__init__(reduction = 'auto', name = \"FocalTverskyLoss\")\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = K.flatten(y_pred)\n",
    "        y_true = K.flatten(y_true)\n",
    "\n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = K.sum((y_pred * y_true))\n",
    "        FP = K.sum(((1-y_true) * y_pred))\n",
    "        FN = K.sum((y_true * (1-y_pred)))\n",
    "\n",
    "        Tversky = (TP + self.smooth) / (TP + self.alpha * FP + (1-self.alpha) * FN + self.smooth)  \n",
    "        \n",
    "        return K.pow((1 - Tversky), self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class CategoricalFocalLoss(tf.losses.Loss):\n",
    "    \"\"\"\n",
    "    식 : loss = - y_true * alpha * ((1 - y_pred)^gamma) * log(y_pred)\n",
    "        \n",
    "    alpha: the same as weighting factor in balanced cross entropy, default 0.25\n",
    "    gamma: focusing parameter for modulating factor (1-p), default 2.0\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, num_classes = num_classes):\n",
    "        super(CategoricalFocalLoss, self).__init__(reduction = 'auto', name = \"CategoricalFocalLoss\")\n",
    "        self._gamma = gamma\n",
    "        self._alpha = np.array([[alpha for _ in range(num_classes) ]], dtype = np.float32)\n",
    "        self._epsilon = 1e-07\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.nn.softmax(y_pred)\n",
    "        y_pred = K.clip(y_pred, self._epsilon, 1. - self._epsilon)\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        loss = self._alpha * K.pow(1 - y_pred, self._gamma) * cross_entropy\n",
    "        return K.mean(K.sum(loss, axis = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class CategoricalCrossEntropy(tf.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(CategoricalCrossEntropy, self).__init__(reduction = 'auto', name = \"CategoricalCrossEntropy\")\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.nn.softmax(y_pred)\n",
    "        return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "aGt6PGTA906W"
   },
   "outputs": [],
   "source": [
    "def run_training(norm, batch_size = batch_size, epochs = epochs):\n",
    "    model = create_model(norm = norm)\n",
    "    loss_fn = DiceLoss()\n",
    "    #loss_fn = CategoricalFocalLoss()\n",
    "    #loss_fn = CategoricalCrossEntropy()\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=lr, weight_decay=weight_decay\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer = optimizer,\n",
    "        loss = loss_fn,\n",
    "        metrics=[\"accuracy\", Dice],\n",
    "    )\n",
    "\n",
    "    \n",
    "    callbacks_list = [tf.keras.callbacks.ModelCheckpoint(filepath = f\"C:/Users/user/Desktop/TransUNet/TransUNet_3D/TransUNet_3D_{norm}\" + \"{epoch}_.h5\",\n",
    "                                                         monitor = \"val_Dice\",\n",
    "                                                         save_best_only = True,\n",
    "                                                         save_weights_only = True,\n",
    "                                                         verbose = 1,\n",
    "                                                         mode = \"max\"\n",
    "                                                        )\n",
    "                     ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data = valid_dataset,\n",
    "        batch_size = batch_size,\n",
    "        epochs = epochs,\n",
    "        callbacks = callbacks_list,\n",
    "        shuffle=True,\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-8427b432541e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"bn\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-75-df7e4ccd19a3>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(norm, batch_size, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiceLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#loss_fn = CategoricalFocalLoss()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#loss_fn = CategoricalCrossEntropy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-6d5c91b1170f>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(norm, image_width, image_height, image_depth, image_channel, num_patches, num_heads, HiddenSizeD, Transformer_MLP_unit, ClassHead_MLP_unit)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_patches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Create a multi-head attention layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         attention_output = layers.MultiHeadAttention(\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mnum_heads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHiddenSizeD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         )(x1, x1)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention'"
     ]
    }
   ],
   "source": [
    "history = run_training(norm = \"bn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcd4914b"
   },
   "source": [
    "# 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-be9fab4aecbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnew_test_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"blur\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"gamma\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"flip\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"noise\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"rot\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"shear\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"translate\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mnew_test_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_name' is not defined"
     ]
    }
   ],
   "source": [
    "new_test_name = []\n",
    "for name in test_name:\n",
    "    if (\"blur\" not in name) and (\"gamma\" not in name) and (\"flip\" not in name) and (\"noise\" not in name) and (\"rot\" not in name) and (\"shear\" not in name) and (\"translate\" not in name):\n",
    "        new_test_name.append(name)\n",
    "        \n",
    "print(len(new_test_name),\"\\n\",new_test_name)\n",
    "test_name = new_test_name\n",
    "del new_test_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(names = test_name):\n",
    "    for name in names:\n",
    "        x_dir = f\"{base_dir}/train/Data_3/{name}\"\n",
    "        y_dir = f\"{base_dir}/train/Mask_3/{name}\"\n",
    "\n",
    "        x = np.load(x_dir)\n",
    "        y = np.load(y_dir)\n",
    "        yield (x, y)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(test_generator,\n",
    "                                               (tf.float32, tf.float32),\n",
    "                                               (tf.TensorShape([image_height, image_width, image_depth, image_channel]), tf.TensorShape([image_height, image_width, image_depth, num_classes])),\n",
    "                                             )\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "model = create_model()\n",
    "loss_fn = DiceLoss()\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=weight_decay)\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = loss_fn,\n",
    "              metrics=[\"accuracy\", Dice])\n",
    "\n",
    "model.load_weights(f\"C:/Users/CBNUH/Desktop/Transformer/2. TransUNet/TransUNet_3D/TransUNet_3D.h5\")\n",
    "_, _, dice = model.evaluate(test_dataset, verbose = 1)\n",
    "print(f\"Test dice: {round(dice * 100, 2)}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_generator(test_generator,\n",
    "                                              (tf.float32, tf.float32),\n",
    "                                              (tf.TensorShape([image_height, image_width, image_depth, image_channel]), tf.TensorShape([image_height, image_width, image_depth, num_classes])),\n",
    "                                             )\n",
    "\n",
    "model.load_weights(f\"C:/Users/CBNUH/Desktop/Transformer/2. TransUNet/TransUNet_3D/TransUNet_3D.h5\")\n",
    "\n",
    "for i, test in enumerate(test_dataset):\n",
    "    data, mask = test[0], test[1]\n",
    "    plot_slices(data)\n",
    "    data = tf.reshape(data, [1, image_width, image_height, image_depth, image_channel])\n",
    "    \n",
    "    mask = tf.math.argmax(mask, -1)\n",
    "    mask = tf.reshape(mask, [image_width, image_height, image_depth, image_channel])\n",
    "    plot_slices(mask, vmin = 0, vmax = 2)\n",
    "    \n",
    "    pred = model.predict(data)\n",
    "    pred = pred[0]\n",
    "    pred = tf.math.argmax(pred, -1)\n",
    "    pred = tf.reshape(pred, [image_width, image_height, image_depth, image_channel])\n",
    "    plot_slices(pred, vmin = 0, vmax = 2)\n",
    "\n",
    "    if i == 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83ceebe1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "2.2. TransUNet_3D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
